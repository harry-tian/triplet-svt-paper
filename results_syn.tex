%!TEX root = main.tex



\section{Synthetic Experiment}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/wv/wv.pdf}
   \caption{VW dataset. (a) shows the non-linear decision boundary of the dataset determined by two features: the head and the body size of the fictional insects. The Weevil has a mid-sized body and mid-sized head, while the Vespula does not. Tail length and texture are two non-informative features.}
   \label{fig:vw}
  \end{figure}

To understand the strengths and limitations of our method, we first experiment with synthetic datasets.
Using simulated human similarity metrics, we control and vary the level of disagreement between the classification groundtruth and the human's knowledge.

\subsection{Synthetic dataset and simulated humans}

We use the synthetic dataset ``Vespula vs Weevil'' (VW) from \cite{chen2018near}.
It is a binary image classification dataset of two fictional species of insects.
Each example contains four features, two of them---head and body size---are predictive of the label, the other two---tail length and texture---are completely non-predictive.
We generate 2000 images and randomly split the dataset into training, validation, and testing sets in a 60\%:20\%:20\% ratio.
The labels are determined various synthetic decision boundaries, such as the one shown in \figref{fig:vw}a.
% \shi{move this sentence to results part}
We report the results for \figref{fig:vw}a in the main paper; see the appendix for full results.

To generate triplets data, we simulate human visual similarity metrics by adding weights to each feature in Euclidean distance computation.
By changing the weight on each feature, we can control the level of disagreement between simulated human and the groundtruth.
All procedures that involve humans (e.g., triplet data collection and evaluation) are replaced by the simulated human in this set of experiments.

To quantify the disagreement, we use 1-NN classification accuracy following the simulated human similarity metric; we refer to it as the alignment score.
The alignment score ranges from 50\% (setting the informative features' weights to 0 and distractor weights to 1) to 100\% (setting all weights to 1).
See appendix for more details on how we generate these weights.
In each setting, we generate 40,000 triplets using the simulated humans similarity metric.
% \chenhao{explain the choice of weights in more details and refer to the appendix.}
% \harry{TODO (hard to explain our selection, it is quite arbitrary)}
% \chenhao{explain the search process at least, explain at least how you control the four features}

% \para{Hyperparameters. } For VW we oberve that using a 512-dimension embedding provides better performance across all evaluation metrics and all models. Other details can be found in appendix.
% \harry{do we need more here? batch size, learning rate etc.}
% \chenhao{not here, but we should say it at other places and include all these in the appendix.}
% \harry{where's a good place?}\cc{appendix}

\subsection{Results}

% 
% We think of both as functions: the human intuition is a distance metric representing the human judgement of visual similarity, and the classification groundtruth is a partition function that matches each example in the input space to a label.
% By restricting the complexity of both functions, we can validate the \mtl model's ability to achieve a good trade-off between high classification accuracy and aligning with human intuition.
% \shi{The connection between zone of proximal development and the gap between decision support and training.}

% \para{\mtl achieves comparable classification and triplet accuracy.} We show in \figref{fig:wv-acc} that 
% \mtl achieves high classfication accuracy compared to RESN and high triplet accuracy compared to TN (results are included in the appendix). 
% \cc{we need to include this result in the main paper not appendix (?)}
% \chenhao{this should be at the beginning when we explain our table and what model we compare with. We do not need too many sentences here, but add numbers.}

We compare \mtl, \resn, \tn on classification accuracy, triplet accuracy, and decision support performance for simulated humans. We train all three models with a large emebdding dimension of 512 and a small emebdding dimension of 50 and observe that a 512-dimension embeddings is preferable based on most metrics. We also train \mtl on a filtered vs. unfiltered triplets as well as with different values $\lambda$. 
For our main results, we report the performance with $\lambda=0.5$ and filtered triplets.
We will discuss the effect of filtering later in this section.
$\lambda$'s role is relatively limited and we will discuss its effect in appendix.

% We report the filtered condition decision support performance, but unfiltered triplets do have other advantages as will be discussed later in this section. 
% We report the results $\lambda=0.5$, but .



% Table~\ref{tab:main-results} compares \mtl, \resn, and \tn on classification accuracy, triplet accuracy, and decision support performance (for simulated humans). Based on decision support accuracy, we chose our best \mtl model which uses fitlered triplets and a 512-dimension embedding.

% \shi{explain how you chose the filtered / unfiltered models. which metric did you use?}
% We compare our filtered \mtl to two baselines \resn and unfiltered \tn.

% More detailed results on \mtl classification and triplet accuracies are included in the appendix.

In synthetic experiments, \mtl achieves the same perfect classification accuracy as \resn (100\%), and a triplet accuracy of 0.968 which is comparable to \tn.
This shows that \mtl indeed learns both the classification task and human similarity prediction task.
We next present the evaluation on case-based decision support with synthetic humans, which is the key goal of this work.

% We start by discussing results with our best \mtl model in different case-based decision support scenarios, and then discuss the effect of different parameter choices. 
% We evaluate \mtl using H2H comparison with \resn as report the results of using NI and NO as justifications.
% We also present NN decision support and NIFO decision support across different alignment settings in comparison to \resn and \tn.

\para{\mtl significantly outperforms \resn in H2H.} 
% \chenhao{replace the header with the main takeaway.}
Our synthetic humans prefer \mtl over \resn by a large margin as justifications for both nearest in-class examples and nearest out-of-class examples, indicating the NIs and NOs selected from the \mtl representations are more aligned with the synthetic humans than \tn. % There is no clear pattern between H2H performance and alignment, suggesting that \mtl is able to learn a wide range of human similarity. 
% Also note that 
For NI H2H, the preference towards \mtl declines as the alignment improves, because if alignment between human similarity and classification increases, \resn can capture human similarity as a byproduct of classification.
Also, NO H2H is higher than NI H2H, suggesting \resn learns a better representation within each class compared to between classes. 
% \harry{I want to say something like "\resn is likely to produce embeddings with class-defined clusters, making NI easier than NO". not sure if that makes sense}.

% \mtl learns a more human-compatible embedding space.


% \cc{from important results to unimportant results}

\para{\mtl provides the best decision support.} 
% \harry{should we bold the highest numbers in tbale1?}
Table \ref{tab:main-results} shows that \mtl achieves the highest NINO and NIFO decision support scores in all alignments. %This further emphasizes that the \mtl representation has high classificaiton ability and human similarity alignment. 
% \harry{TODO: more discussion here.}
In NINO decision support, \resn consistently outperforms \tn, highlighting that representation solely learned for metric learning is ineffective for decision support.
For all models, the decision support performance improves as the alignments increases, suggesting that decision support is easier when human similarity judgement is aligned with the classification task.
\resn and \tn are more comparable in NIFO, while \mtl consistently shows 100\%.
The fact that \resn shows comparable performance between NINO and NIFO further confirms that \resn does not capture meaning similarity for examples from two different classes.

% both \resn and \tn see decerased decision support performance in varying settings. \resn sees good NINO performance but fails at the easier NIFO task; its NINO and NIFO scores are similar, likely because \resn embeddings do not differentiate NO and FO. \tn is good at NIFO but significantly worse at NINO, suggesting that NINO may be a task that benefits from strong classification performance and high human alignment. 

\para{Filtering triplets leads to better decision support}. \figref{fig:filter} shows that filtering class-inconsistent triplets improves \mtl's decision support performance across all alignments. Further details in the supplementary material show that filtering slightly hurts direct comparison performance. 
This suggests that in terms of decision support, the benefit of filtering out human noise may overweigh the loss of some similarity judgment.

% \para{Alignment matches with NINO decision support.} For all models, there is a linear trend between alignment and NINO decision support performance. 


\input{tables/main-results.tex}

\begin{table}[t]
  \begin{minipage}[b]{0.38\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/filtered_NINO_err.pdf}
    % \begin{subfigure}[b]{0.495\textwidth}
    % \includegraphics[width=\textwidth]{figures/filtered_h2h.pdf}
    % \caption{H2H with \resn}
    % \end{subfigure}
    \captionof{figure}{NINO decision support on \mtl with filtered and unfiltered triplets. Filtered triplets leads to greatly improved performance. }
    % \cc{legend is wrong?}}
    \label{fig:filter}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.6\textwidth}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/noise_h2h_v1.pdf}
      % \caption{NI H2H}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/noise_NINO_v1.pdf}
      % \caption{NINO}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/noise_NIFO.pdf}
      % \caption{NIFO}
      \end{subfigure}
      \captionof{figure}{Noise hurts direct comparison more than decision support.}
      \label{fig:noise}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/num_h2h_v1.pdf}
        % \caption{H2H with \resn}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/num_NINO_v1.pdf}
        % \caption{NINO}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/num_NIFO_v1.pdf}
        % \caption{NIFO}
        \end{subfigure}
        \caption{\mtl performance declines as the number of triplets decreases, but shows strong NIFO decision support accuracy even with very few triplets.}
        % \harry{TODO: update to predicted label}}
        \label{fig:vary-num}
  \end{minipage}
\end{table}


\para{The effect of noise in triplets.}
To test the limitations of \mtl, we perturb human judgment by adding noise to triplets. 
We add noise to triplets by randomly flipping the similarity judgement (i.e., a triplet $(x^r, x^+, x^-)$ becomes $(x^r, x^-, x^+)$) with probability $p$.
As shown in \figref{fig:noise}, direct comparison results decreases linearly as noise increases, decision support performance does not start decreasing util $p=0.5$. 
This is not as surprising for two reasons. First, since VW is a binary dataset, half of the triplets are ones where $x^+, x^-$ belong to the same class; 
flipping these triplets does not have a significant effect on the generated embeddings. 
More importantly, filtering greatly reduces the adversarial effect of adding noise, highlighting the importance of filtering. 
In comparison, \tn drops linearly in NINO, so do \mtl with unfiltered triplets (see appendix).

% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_h2h.pdf}
%   \caption{NI H2H with \resn}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_NINO.pdf}
%   \caption{NINO decision support}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_NIFO.pdf}
%   \caption{NIFO decision support}
%   \end{subfigure}
%   \caption{Results on adding noise to triplets. Noise hurts direct comparison more than decision support. \harry{TODO: update to predicted label}}
%   \label{fig:noise}
% \end{figure}


% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_h2h_unfiltered.pdf}
%   \caption{NI H2H with \resn}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_NINO_unfiltered.pdf}
%   \caption{NINO decision support}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/noise_NIFO_unfiltered.pdf}
%   \caption{NIFO decision support}
%   \end{subfigure}
%   \caption{Results on adding noise to unfiltered triplets. The impact of noise is much more prominent when using unfiltered triples.}
%   \label{fig:noise_unfiltered}
% \end{figure}

% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/num_h2h.pdf}
%   \caption{H2H with \resn}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/num_NINO.pdf}
%   \caption{NINO}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.329\textwidth}
%   \includegraphics[width=\textwidth]{figures/num_NIFO.pdf}
%   \caption{NIFO}
%   \end{subfigure}
%   \caption{Results on decreasing the number of triplets. \mtl shows decent performance with very few triplets.\harry{TODO: update to predicted label}}
%   \label{fig:vary-num}
% \end{figure}

\para{Number of triplets} We examine the effect of the number of triplets. We decrease number of triplets by powers of 2 
and find that H2H preference towards \mtl indeed declines as \mtl representation is less human-compatible with fewer training data.
As for decision support, in NINO, \mtl declines and eventually approaches \resn except the outlier at end, while in NIFO, \mtl is able to stay 100\% even as the number of triplets declines. 

% \harry{TODO: more here}
% \harry{The results are honestly quite weird and counterintuitive... do we stil want to show these?}
% \chenhao{not really weird except the final one? you can drop the final point which is indeed low noise scenario, but I think it is fine to keep it too}

