%!TEX root = main.tex 

% \clearpage
\section{Human subject study}
\label{sec:supp_human}

\subsection{Dataset}
Our BM dataset include four species of butterflies and moths including: Peacock Butterfly, Ringlet Butterfly, Caterpiller Moth, and Tiger Moth. An example of each species is shown in Fig \ref{fig:bm-species}.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.24\textwidth}
      \includegraphics[width=\textwidth]{figures/bm/species/ringlet.jpg}
      \caption{Ringlet Butterfly}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/bm/species/peacock.jpg}
        \caption{Peacock Butterfly}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/bm/species/caterpillar.jpg}
        \caption{Caterpiller Moth}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/bm/species/tiger.jpg}
        \caption{Tiger Moth}
    \end{subfigure}
    \caption{An example of each species in the BM dataset.}
    \label{fig:bm-species}
\end{figure}

\subsection{Hyperparameters}
We use different controlling strength between classification and human judgment prediction, including $\lambda$s at 0.2, 0.5, and 0.8.
We use the Adam optimizer \cite{kingma2014adam} with learning rate $1e-4$.
Our training batch size is $120$ for triplet prediction, and $30$ for classification.
% \chenhao{how do you stop? choose the number of epoches?}
All models are trained for 50 epoches. The checkpoint with the lowest validation total loss in each run is selected for evaluations and applications.

\subsection{Classification and Triplet Accuracy}
\input{tables/bm-models.tex}
We present the test-time classification and triplet accuracy of our models in Table \ref{tab:bm-models}. Both \resn and \mtl achieve above 97.5\% classification accuracy. \mtl in the 512-dimension unfiltered setting achieve 100.0\% classification accuracy. Both \tn and \mtl achieve above 70.7\% triplet accuracy. Both \tn model and \mtl achieve the highest triplet accuracy in the 50-dimension unfiltered setting with triplet accuracy at 75.9\% and 76.2\% respectively.

We also evaluate the pretrained LPIPS metric \cite{zhang2018perceptual} on our triplet test set as baselines for learning perceptual similarity.
Results with AlexNet backbone and VGG backbone are at 54.5\% and 55.0\% triplet accuracy respectively, suggesting that \tn and \mtl provides much better triplet accuracy in this task compared to a generic model.


\subsection{Justification and Decision Support}

% One possible strategy to examine the quality of representations is to ask people to subjectively rate similarity between the test image and the justification. However, self-reported ratings are known to be problematic. \chenhao{add cite} 
\subsubsection{Evaluation setup for Justification}
To derive objective judgment of the quality of justification, we setup head-to-head comparisons (``\textbf{H2H}'') between two justifications derived from two representations and examine which justification human (or a synthetic agent) considers more similar.
In addition to the typical justification for the predicted label, we also examine that for the other class as those examples will be used in decision support.
We refer to the nearest example with the predicted label as {\em NI}, and nearest example in the other class as {\em NO}.

\subsubsection{Results with a synthetic agent}
To run synthetic experiments for case-based decision support, we select the \tn with the best test triplet accuracy as our synthetic agent, and then evaluate the examples produced by all representations.
% We do not compare with \tn as \tn is the synthetic agent.

\han{h2h results here is removed}
\para{\mtl is prefered over \resn in H2H.}

We compare examples selected from different models in different configurations to examples selected by the \resn baseline with the same dimensionality. 

Table~\ref{tb:bm_h2h} shows how often the synthetic agent prefers the tested model examples to baseline \resn examples.

In all settings, the preference towards \mtl is above 50\%, but not as high as those in our synthetic experiments with the VW dataset.
Filtering out class-inconsistent triplets improves the preference for the nearest example with the predicted label, while hurting the preference for the nearest out-of-class example.

\han{decision results with syn agent is removed}
\para{Decision support simulations shows a large dimension benefits \resn but hurts unfiltered \mtl in NINO.}
We also run simulated decision support with the \tn synthetic agent. Table \ref{tb:bm_decision} shows decision support accuracy for different settings. 
\resn have both higher NINO decision support accu and NIFO scores when we use a large dimension at 512. We hypothesize that for \resn, reducing dimension may force the network to discard dimensions useful for human judgments but keep dimensions useful for classification. We then use the 512-dimension \resn representations with the highest intrinsic evaluation scores as our \resn baseline in later studies.


For \mtl, NINO decision support accuracy are in general comparable to 87.5\% score of the 512-dimension \resn baseline except unfiltered 512-dimension \mtl which has only 80\%. 
We hypothesize that representations of large dimension may struggle more with contradicting signals between metric learning and supervised classification in the unfiltered settings.
For NIFO, \mtl achieves perfect scores in all settings.


Overall, to proceed with our human-subject experiments, we choose \mtl filtered with 50 dimensions as our best \mtl model as it achieves a good balance between H2H and NINO decision support.
For \resn, we choose the model with 512 dimensions.
We conduct head-to-head comparison between these two models.
Our synthetic agent prefers \mtl in 70\% of the nearest in-class examples and in 97.5\% of the nearest out-of-class examples.

\begin{table}[t]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \input{tables/bm-h2h.tex}
        \captionof{table}{BM H2H preference results with synthetic agent.}
        \label{tb:bm_h2h}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \input{tables/bm-decision-transposed.tex}
        \captionof{table}{BM decision support accuracy with synthetic agent.
        }
        \label{tb:bm_decision}
    \end{minipage}
\end{table}

\subsubsection{Human experiments with H2H comparisons}
\para{H2H comparison results show \mtl NI examples are slightly but significantly preferred over \resn NI examples according to human visual similarity.}
We recruit 30 Prolific workers to make H2H comparisons between \mtl NI examples and \resn NI examples. The mean preference for \mtl over \resn is 0.5316 with a 95\% confidence interval of $\pm0.0302$ ($p=0.0413$ with one-sample t-test). 
This means the \mtl NI examples are closer to the test images than \resn NI examples with statistical significance according to human visual similarity.  
% This result is on par with the 60\% preferred rate based on our synthetic agent.

\subsection{Subject Evaluations}
We include survey questions in the end of the decision support tasks. 
The two required questions are: 
1) accuracy belief: ``How many questions do you think you have answered correctly?'' 
2) support usefulness: ``Do you agree that the reference images are helpful when you decide the class of the test image?'' This is measured in a 5-point Likert scale.
Results are reported in Fig \ref{fig:subjective}. 

Subjective beliefs of accuracy display a similar trend as that in the objective measure of accuracy. \mtl NINO and \mtl NIFO are significantly better than RIRO, \resn NINO, and \resn NIFO. However, the differences among RIRO, \resn NINO and \resn NIFO are no longer significant.

Subjective beliefs of decision support usefulness display the same trend as that in the subjective measure of accuracy beliefs. \mtl NINO and \mtl NIFO are believed to be more useful than other decision supports, among which there is no significant difference.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/bm/prolific/decision-acc_belief.pdf}
        \caption{Subjective evaluation of participants' beliefs on their accuracy.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/bm/prolific/decision-usefulness.pdf}
        \caption{Subjective evaluation of participants' beliefs on decision support usefulness.}
    \end{subfigure}
    \caption{Subjective evaluation in the decision support task.}
    \label{fig:subjective}
\end{figure}


% \clearpage
\subsection{Interface}
We present the screenshots of our interface in this section. 
Our interface consists of four stages. 
Participants will see the consent page at the beginning, as shown in Fig \ref{fig:interface_consent}. 
After consent page, participants will see task specific instructions, as shown in Fig \ref{fig:interface_prolific}. 
After entering the task, partipants will see the questions, as shown in Fig \ref{fig:interface_questions}. 
We also include two attention check questions in all studies to check whether participants are paying attention to the questions. 
Following suggestions on Prolific, we design the attention check with explicit instructions, as shown in Fig \ref{fig:interface_attention}.
After finishing all questions, participants will reach the end page and return to Prolific, as shown in Fig \ref{fig:interface_end}. 
Our study is reviewed by the Internal Review Board (IRB) at our institution with study number that we will release upon acceptance to preserve anonymity.
% IRB22-0388. 
% \han{Will we be identified through the IRB number?}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/interface/consent-annotate.pdf}
    \caption{The consent form page on our interface.}
    \label{fig:interface_consent}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/prolific-annotate.pdf}
        \caption{The annotation and head-to-head comparison task instructions.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/prolific-decision.pdf}
        \caption{The decision support task instructions.}
    \end{subfigure}
    \caption{The task-specific instruction page on our interface.}
    \label{fig:interface_prolific}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/annotate.pdf}
        \caption{The annotation and head-to-head comparison task questions.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/decision.pdf}
        \caption{The decision support task questions.}
    \end{subfigure}
    \caption{The task-specific questions on our interface.}
    \label{fig:interface_questions}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/attention-annotate.pdf}
        \caption{The annotation and head-to-head comparison task attention check questions.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/attention-decision.pdf}
        \caption{The decision support task attention check questions.}
    \end{subfigure}
    \caption{The task-specific attention check questions on our interface.}
    \label{fig:interface_attention}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/interface/survey-decision.pdf}
    \caption{The survey page of the decision support task on our interface.}
    \label{fig:interface_survey_decision}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/end-annotate.pdf}
        \caption{The annotation and head-to-head comparison task end page.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/interface/end-decision.pdf}
        \caption{The decision support task end page.\\ }
    \end{subfigure}
    \caption{The task-specific end page on our interface.}
    \label{fig:interface_end}
\end{figure}



\subsection{Crowdsourcing}
We recruit our participants on a crowdsourcing platform: Prolific (www.prolific.co) [April-May 2022].
We conduct three total studies: an annotation study, a decision support study, and a head-to-head comparison study.
We use the default standard sampling on Prolific for participant recruitment.
Eligible participants are limited to those reside in United States.
Participants are not allowed to attempt the same study more than once.

\para{Triplet annotation study}
We recruit 90 participants in total. We conduct a pilot study with 7 participants to test the interface, and recruit 83 participants for the actual collection of annotations. 3 participants fail the attention check questions and their responses are excluded in the results. We spend in total \$76.01 with an average pay at \$10.63 per hour. The median time taken to complete the study is 3'22''.

% ap1 12 9.34, ab1 14.55 10.67, ab2 10.97 24, ab3 9.4 32.
% median time 202.199s

\para{Decision support study}
We recruit 161 participants in total. 3 participants fail the attention check questions and their responses are excluded in the results. We take the first 30 responses in each conditon to compile the results. We spend in total \$126.40 with an average pay at \$9.32 per hour. The median time taken to complete the study is 3'53''.

% median time 232.611s


\para{Head-to-head comparison study}
We recruit 31 participants in total, where 1 participant fail the attention check questions and their responses are excluded in the results. We spend in total \$24.00 with an average pay at \$9.40 per hour. The median time taken to complete the study is 3'43''.

% median time 223.385s
