%!TEX root = main.tex

\section{Related Work}
\para{Connection to machine teaching}
Explanation-based teaching captures a broad class of machine teaching problems where labels provided by the teacher are coupled with additional information (such as highlighting regions or features on an image, supplementing the teaching instruction with contrastive examples \citep{wang2021teaching}). Explanations were shown to be effective in guiding a human learner to improve classification performance by steering the learner's attention \citep{grant2003,roads2016,chen2018near,macaodha18teaching}.
We note that the case-based decision support considered in this paper is intrinsically connected to explanation-based machine teaching in that both problems seek a set of (labeled) examples as instructions to improve human decision making, with two fundamental differences that span both problem setup and objective: (a) for case-based decision support, we assume human is the decision maker and only consult the model as an external aid when making predictions; the model always presents, even at the prediction phase. In contrast, classical machine teaching aims to distill the model fully to the human at the training phase, and human will make their decisions without the model at the prediction phase. (b) Due to such difference in problem setups, the best performance in case-based decision support is natually constrained and grounded by the human's (intial) model---as a \textit{nonparametric} decision model (e.d. 1-NN) conditioning on the provided decision support---and the focus is on the joint design of the representation and the example selection policy. % with extra consideration in human's trust of the model.
While most machine teaching work consider a fixed representation of (human) learners, it often assumes a \textit{parametrized} decision model of the learner, and the focus is on the design of an example selection policy that steers the learner to adopt the best parameterization.


\para{Ordinal embedding}
The ordinal embedding problem \citep{ghosh2019landmark,van2012stochastic,kleindessner2017kernel,kleindessner2014uniqueness,terada2014local,park2015preference} aim to find low-dimensional representations that respect ordinal feedback. Currently there exist several techniques for ordinal embedding. Generalized Non-metric Multidimensional Scaling (GNMDS) \citep{agarwal2007generalized} takes a max-margin approach by minimizing hinge loss. Stochastic Triplet Embedding (STE) \citep{van2012stochastic} assumes the Bradley-Terry-Luce (BTL) noise model \citep{bradley1952rank,luce1959individual} and minimizes logistic loss. The Crowd Kernel \citep{tamuz2011adaptively} and t-STE \citep{van2012stochastic} propose alternative non-convex loss measures based on probabilistic generative models. These results are primarily empirical and focus on minimizing prediction error on unobserved triplets. In principle, one can plugin these approaches to the \mtl model as alternatives to the triplet margin loss \ref{eq:mtl_loss}.
% \yuxin{can someone look into this paper \citep{nadagouda2022active} and include it as a reference?}
% >>>>>>> 46bc1c2482fcbd751baa2628e7249d54c9268b2b
%rely on expensive gradient or projection computations and are unsuitable for large datasets. The results in these papers are primarily empirical and focus on minimizing prediction error on unobserved triplets.
