%!TEX root = main.tex
% \section{Metric Learning for Case-Based Decision Support}
\section{Case-Based Decision Support}
\label{sec:formulation}



Consider the problem of using a classification model $h: \inputspace \rightarrow \outputspace$ as decision support for humans.
Simply showing the predicted label from the model provides limited information and ``explanations'' are commonly hypothesized to improve human performance \citep{doshi2017towards}.
We focus on information presented in the form of examples from the training data, also known as case-based decision support \cite{kolodneer1991improving,begum2009case,liao2000case,angehrn1998case}.
Case-based decision support can have diverse use cases and goals.
Given a test example ($x$) and its predicted label ($\hat{y}$), two common use cases are:
\begin{itemize}[topsep=0pt, leftmargin=*, itemsep=0pt]
  \item Presenting the nearest neighbor of $x$ with label $\hat{y}$ as a justification of the predicted label. We refer to this scenario as {\em justification}.
  \item Presenting the nearest neighbor in each class without presenting $\hat{y}$. This approach makes a best-effort attempt to provide evidence for each class and leaves the final decision to human, without biasing human with the predicted label. We refer to this scenario as {\em decision support}.
\end{itemize}

\paragraph{Formulation.}
In this work, we formalize the problem of case-based decision support. 
The goal is to assist humans on a classification problem with groundtruth $\f: \inputspace \rightarrow \outputspace$.
We assume access to a representation model $g$, which takes an input $x$ and generates an $m$-dimensional representation $g(x) \in R^m$.
For each test instance $x$, an example selection policy $\pi$ chooses $k$ labeled examples from the training set $D^\text{train}$ and show them to the human (optionally along with the labels); the human then makes a prediction by choosing a label from $\outputspace$.
%% CT-0517: this is not essential?
% Without loss of generality, we will assume that the classification problem is binary from now on.
% In our definition, the main role of the model $g$ is to provide embeddings as inputs to the example selection policy, not making predictions autonomously, so we do not have to obtain representations from a classification model.
% That said, we use representation in a supervised classification model as our main baseline because it is the defacto choice in practice. 
% \chenhao{add citation}
As discussed in the two common use cases, example selection policy tends to be some form of nearest neighbor since it is important to be interpretable.
The focus of this work is thus on the effectiveness of $g$ for case-based decision support.

% \chenhao{not sure we need this paragraph}
% We refer to the embedding model $g$ (or \textit{model} for short) and the example selection policy $\pi$ (or \textit{policy} for short) collectively as the decision support system (or \textit{the system} for short).

Given a classification model $h$, the representation in justification and decision support is a byproduct derived from $h$, which is the last layer before the classification head.
We refer to this model as $e(h)$.\footnote{In general, we can use the representation in any layer, but in preliminary experiments, we find representation from the last layer is most effective.}
In justification, the example policy is $NN(x, e(h), D^\text{train}_{\hat{y}})$, where $D^\text{train}_{\hat{y}}$ refers to the subset of training data with label $\hat{y}$ and $NN$ finds the nearest neighbor of $x$ using representations from $e(h)$ among the subset of examples with label $\hat{y}$.
In decision support, the example policy is $\{NN(x, e(h), D^\text{train}_{y}), ~\forall y \in \outputspace \}$.

\han{removed misalignment paragraph here.}
% \paragraph{Misalignment with human similarity metric is detrimental.}
% To reason about the effectiveness of a representation model, we need to think about the goal of case-based decision support.
% Let us start with justification, which is a relatively easy case. 
% To justify a predicted label, the chosen example should ideally {\em appear similar} to the test image.
% Crucially, this similarity is perceived by humans and the example policy selects the nearest neighbor based on model representation.
% The gap between human representation and model representation (\figref{fig:model_vs_human}) leads to undesirable justification.

% Decision support, however, represents a more complicated scenario.
% We start by emphasizing that the goal is not simply to maximize human decision accuracy, because that goal can invite policies that may intentionally show examples that are far away to nudge or deceive human towards making a particular decision.
% Choosing nearest neighbors in each class is thus an attempt to present the most reliable evidence from the representation model so that humans can make their own decisions, hence preserving their agency.
% Therefore, the chosen nearest neighbors should be visually similar to the test instance by human perception, again highlighting the potential gap between model representation and human representation.
% Assuming that human follow the natural strategy by picking the presented instance that's most {\em similar} to the test instance and answering with the corresponding label, then ideally, nearest neighbors in each class retain key information useful for classification so that they can reveal the separation learned in the model.

% In both cases, aligning model representations with human similarity metric is crucial for case-based decision support; we refer to it as the \textit{metric alignment problem}.
% It is unlikely that we will get high alignment by default even when the model's classification accuracy is comparable to human.
% Models trained with supervised learning almost always exploit patterns in the training data that are
% \begin{enumerate*}[label=(\roman*)]
%   \item not robust to distribution shifts, and
%   \item counterintuitive or even unobservable for humans~\citep{ilyas2019adversarial,xiao2020noise}
% \end{enumerate*}.

% HAN: below are already commented in NeurIPS submission.
% When we are using more capable models to assist less capable human users, e.g., using an image classifier to assist radiology residents at breast cancer screening~\citep{wu2019deep,schaffter2020evaluation}, the human's incomplete knowledge about the task leads to a different kind of misalignment where the model relies on some legitimate feature that the human isn't aware of~\citep{kolodneer1991improving}.
% In both cases, the misalignment between the model and the human's similarity metric can lead to ineffective case-based decision support; 
% \shi{add another illustrative figure here?}

\paragraph{Combining metric learning on human triplets with supervised classification.}
We propose to address the metric alignment problem with additional supervision on the human similarity metric.
We collect data in the form of human similarity judgment triplets (or \textit{triplets} for short).
Each triplet is an ordered tuple: $(x^r, x^+, x^-)$, which indicates $x^+$ is judged by human as being closer to the reference $x^r$ than $x^-$~\citep{balntas2016learning}.

Given a triplet dataset $T$ and labeled classification dataset $D$, we use triplet margin loss~\citep{balntas2016learning} in conjunction with the cross-entropy loss, controlled by a hyperparameter $\lambda$:
\harry{I removed the equation numbers}
\begin{align*}
  &L = \lambda L_\mathrm{CrossEntropy(CE)} + (1-\lambda) L_\mathrm{TripletMargin(TM)} \label{eq:mtl_loss} \\
  &L_\mathrm{CE} = - \sum_{(x,y)\sim D} \log\left(p_\theta(y|x)\right) \\
  &L_\mathrm{TM} = \sum_{(x^r,x^+,x^-)\sim T} \max\left(d_\theta(x^r,x^+)-d(x^r,x^-)+\mu,0\right)
\end{align*}
% \begin{equation}
% \underbrace{\left[- \sum_{(x,y)\sim D} \log\left(p_\theta(y|x)\right)\right]}_\textrm{Cross-entropy loss} + (1-\lambda)\underbrace{\left[\sum_{(x^r,x^+,x^-)\sim T} \max\left(d_\theta(x^r,x^+)-d(x^r,x^-)+\mu,0\right)\right]}_\textrm{Triplet margin loss}
% \label{eq:mtl_loss}
% \end{equation}

where $d(\cdot,\cdot)$ is the similarity metric based on model representations, and $\mu$ is the margin hyperparameter; we use Euclidean distance and always set $\mu=1$.
We parameterize $\theta$ with a pretrained \resn~\citep{he2016deep}.
When $\lambda=1$ and the triplet margin loss is turned off, the model reduces to a finetuned \resn.
When $\lambda=0$ and the cross-entropy loss is turned off, the model reduces to \citet{balntas2016learning}'s mode; we call it \tn and treat it as a baseline.
We refer to our model as \mtl which stands for \textbf{M}ulti-\textbf{T}ask \textbf{L}earning.

% ; \mtl$_{0.2}$ refers to the \mtl model with $\lambda=0.2$
% Han: no longer using mtl_lambda in results.

% \chenhao{we should emphasize that we are learning representation and leave the selection of example policy to different work.}

% \subsection{Example selection policies} \yuxin{seems to be more appropriate to show up in the experiment section}
% 
% \para{AI model and baselines.}
% 
% \begin{itemize}[topsep=0pt,leftmargin=*,itemsep=-1pt]
%   \item \underline{\resn}~\citep{he2016deep}:
%   We use pretrained \resn model and finetuned on the downstream tasks.
%   \item \underline{\tn}: Another baseline we consider is directly learning human similarity measures with triplet margin loss~\citep{balntas2016learning}, as specified below:
%   \begin{equation}
%   L(a,p,n) = \max\{d(a_i,p_i)-d(a_i,n_i)+\text{margin},0\}
%   \end{equation}
%   We use \resn as the backbone of the \tn for fair comparison. Hence, $d(\cdot,\cdot)$ here is the euclidean distance between the \resn backbone embeddings.
%   \item \underline{\mtl}: In order to jointly learn the classification task and the human-compatible representations, we propose \mtl using the \resn backbone and the loss function as follows:
%   \begin{equation}
%   L = \lambda CE() + (1-\lambda) \text{triplet loss}
%   \end{equation}
%   % \item ResNet + MLP
%   % \item ResNet + ProtoNet
%   % \item ResNet + DWAC
%   % \item MTL with different lambda
%   % \item MTL with decision-consistent triplets
%   % \item MTLT with different lambda
%   % \item Triplet-only
%   % \item random
% \end{itemize}
% \harry{do we want to include a random baseline for selecting random decision suppport examples?}\yuxin{i think it's useful to keep random}



% The problem we are interested in this paper is case-based decision support with AI, where AI agent assists human to perform the end-task by retrieving cases that are helpful to human reasoning process. An overview of the setup is as follows: the AI agent learns to perform the underlying task $\g: \inputspace \rightarrow \outputspace$. Then we have a policy $\pi$ to retrieve a useful set of cases $D$ and show to human decision makers. Human ($h$) makes the decision given $D$ and its own intuition.


% \para{AI agent.} More formally, our AI agent consists of a predictor $g$ and a case retrieving policy $\pi$. The predictor $g$ learns the underlying task (mapping from $\inputspace$ to $\outputspace$) in a supervised manner. The retrieving policy $\pi$ is used to retrieve cases (instances) that will be useful to human decision makers.


% \para{Case-based decision support with AI.}


% \para{Human decision maker.} In our setting, we consider two important aspects of a human decision maker: human intuitions and human decision functions.
%
% First, we elaborate what do we mean by human intuitions and articulate our assumptions.
%
% \begin{assumption}[Form of human intuitions]
% Human intuition is defined as the similarity measure $K(\cdot,\cdot):\inputspace\times\inputspace\rightarrow \mathbb{R}_{\ge 0}$ that the human uses to determine the similarity between the two instances from the input space.
% \end{assumption}
%
% Next, we define human decision function $h$, where human decision is given by $h(K,D,x)$, i.e., humans make their decisions based on the specific instance $x$, their intuition (similarity measure $K$), the provided cases ($D$) from AI.
%
%
% \begin{assumption}[Form of human decision function]
% We assume humans make decisions based on nearest neighbors.
% \end{assumption}
%
%
% \para{Objective.}
% The system objective is to minimize human decision error given the AI retrieved cases $D$ and human intuition $K$.
%
% \begin{equation}
% L(g, h) = l(h(K,D,x), y)
% \end{equation}
