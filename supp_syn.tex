%!TEX root = main.tex 

% \clearpage
\section{Synthetic experiments}
\label{sec:supp_syn}

In addition to BM, we also experiment with synthetic datasets as we can tune more variables and better understand the strengths and limitations of our method. Using simulated human similarity metrics, we control and vary the level of disagreement between the classification groundtruth and the human's knowledge.

\subsection{Synthetic dataset and simulated humans}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/wv/wv.pdf}
   \caption{VW dataset. (a) shows the non-linear decision boundary of the dataset determined by two features: the head and the body size of the fictional insects. The Weevil has a mid-sized body and mid-sized head, while the Vespula does not. Tail length and texture are two non-informative features.}
   \label{fig:vw}
  \end{figure}

% \begin{table}[t]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%     \includegraphics[width=\textwidth]{figures/supp/weights.png}
%     \end{subfigure}
%     \caption{Histogram of alignments generated by searching informative weights in powers of 2.}
%     \label{fig:align-hist}
% \end{table}

We use the synthetic dataset ``Vespula vs Weevil'' (VW) from \citet{chen2018near}.
It is a binary image classification dataset of two fictional species of insects.
Each example contains four features, two of them---head and body size---are predictive of the label, the other two---tail length and texture---are completely non-predictive.
We generate 2000 images and randomly split the dataset into training, validation, and testing sets in a 60\%:20\%:20\% ratio.
The labels are determined by various synthetic decision boundaries, such as the one shown in \figref{fig:vw}a.
We report the results for \figref{fig:vw}a, a double-square decision boundary, in section C.2 and results for a linear decision boundary data in section C.3.


To generate triplets data, we simulate human visual similarity metrics by adding weights to each feature in Euclidean distance computation.
By changing the weight on each feature, we can control the level of disagreement between simulated human and the groundtruth.
All procedures that involve humans (e.g., triplet data collection and evaluation) are replaced by the simulated human in this set of experiments.

To quantify the disagreement, we use 1-NN classification accuracy following the simulated human similarity metric; we refer to it as the alignment score.
The alignment score ranges from 50\% (setting the informative features' weights to 0 and distractor weights to 1) to 100\% (setting all weights to 1).
We generate alignment scores by searching through weight combinations of the simulated human visual similarity metrics. We search the weights in powers of 2, from 0 to $2^{10}$, producing a sparse distribution of alignments.
%  (~\figref{fig:align-hist}). Increasing search range to powers of 10 produces smoother distribution, but the weights are also more extreme and unrealistic. We note that the alignment distribution may vary across different datasets. 
 In our experiments we choose weights and alignments to be as representative to the distribution as possible. In each alignment setting, we generate 40,000 triplets using the simulated humans similarity metric.



\subsection{Double-square decision boundary experiment results}


\subsubsection{Hyperparameters} 
We train all models with a large emebdding dimension of 512 and a small emebdding dimension of 50 and observe that, in contrast to the experiments on BM, a 512-dimension embeddings is preferable based on most metrics and models. 
We also train \mtl on a filtered vs. unfiltered triplets as well as with different values $\lambda$ at 0.2, 0.5, and 0.8. 
For our main results, we report the performance with $\lambda=0.5$ and filtered triplets.
We will discuss the effect of filtering later in this section.  


We use the Adam optimizer \cite{kingma2014adam} with learning rate $1e-4$.
We use a training batch size of $40$ for triplet prediction, and $30$ for classification.


\subsubsection{Main results}

\input{tables/supp/clf_triplet_acc.tex}
\input{tables/supp/wv_square_filtered_l=0.5_ci.tex}

\begin{figure}[h!]
      \centering
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/filtered_NINO_err.pdf}
        \caption{NINO decision support}
        \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_square_NI-h2h_filter.pdf}
        \caption{NI-H2H with \resn}
      \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_square_NO-h2h_filter.pdf}
        \caption{NO-H2H with \resn}
      \end{subfigure}
        \caption{Decision support and H2H performance on VW double-square decision boundary data, comparing \mtl with filtered and unfiltered triplets.}
        \label{fig:square-filter}
\end{figure}
  
\begin{figure}[t]
      \centering
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/noise_h2h_v1.pdf}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/noise_NINO_v1.pdf}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/noise_NIFO.pdf}
        \end{subfigure}
        \caption{Results on VW double-square decision boundary data with varying number of triplets. \mtl uses filtered triplets}
        \label{fig:noise}
      \end{figure}

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/supp/wv_square_noise_unfiltered_h2h.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/supp/wv_square_noise_unfiltered_NINO.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/supp/wv_square_noise_unfiltered_NIFO.pdf}
    \end{subfigure}
    \caption{Results on VW double-square decision boundary data with varying number of triplets. \mtl uses unfiltered triplets}
    \label{fig:noise-unfiltered}
\end{figure}

\begin{figure}[h!]
  \begin{subfigure}[b]{0.32\textwidth}
          \includegraphics[width=\textwidth]{figures/num_h2h_v1.pdf}
          \end{subfigure}
          \begin{subfigure}[b]{0.32\textwidth}
          \includegraphics[width=\textwidth]{figures/num_NINO_v1.pdf}
          \end{subfigure}
          \begin{subfigure}[b]{0.32\textwidth}
          \includegraphics[width=\textwidth]{figures/num_NIFO_v1.pdf}
          \end{subfigure}
          \caption{Results on VW double-square decision boundary data with varying number of triplets. \mtl uses unfiltered triplets}
          \label{fig:vary-num}
    \end{figure}

\input{tables/supp/wv_square_filtered_l=0.2_ci.tex}

% \input{tables/supp/wv_square_filtered_l=0.8_ci.tex}

\input{tables/supp/wv_square_filtered_l=0.5_d=50.tex}

% \input{tables/supp/wv_square_unfiltered_l=0.5_ci.tex}

We compare \mtl, \resn, \tn on classification accuracy, triplet accuracy, and decision support performance for simulated humans. 
Table~\ref{tab:wv-square-clf-trip} shows how tuning $\lambda$ affects \mtl 's classification and triplet accuracy. Higher $\lambda$ drives \mtl to behave more simlar to \resn while lower \mtl is more similar to \tn. This shows that \mtl indeed learns both the classification task and human similarity prediction task.


In Table~\ref{tab:table1-ci} we present results with the best set of hyperparameter: filtered triplets, 512-dimension embedding, $\lambda=0.5$. We present our main observations as follows.


\para{\mtl significantly outperforms \resn in H2H.} 
Our synthetic humans prefer \mtl over \resn by a large margin as justifications for both nearest in-class examples and nearest out-of-class examples, indicating the NIs and NOs selected from the \mtl representations are more aligned with the synthetic humans than \tn. 

For NI H2H, the preference towards \mtl declines as the alignment improves, because if alignment between human similarity and classification increases, \resn can capture human similarity as a byproduct of classification.
Also, NO H2H is higher than NI H2H, suggesting \resn learns a better representation within each class compared to between classes. 

\para{\mtl provides the best decision support.} 
Table \ref{tab:table1-ci} shows that \mtl achieves the highest NINO and NIFO decision support scores in all alignments. 
In NINO decision support, \resn consistently outperforms \tn, highlighting that representation solely learned for metric learning is ineffective for decision support.
For all models, the decision support performance improves as the alignments increases, suggesting that decision support is easier when human similarity judgement is aligned with the classification task.
\resn and \tn are more comparable in NIFO, while \mtl consistently shows 100\%.
The fact that \resn shows comparable performance between NINO and NIFO further confirms that \resn does not capture meaning similarity for examples from two different classes.

\para{Filtering triplets leads to better decision support}. \figref{fig:square-filter}(a) shows that filtering class-inconsistent triplets improves \mtl's decision support performance across all alignments. However, filtering does lead to slightly worse H2H performance. This suggests that in terms of decision support, the benefit of filtering out human noise may overweigh the loss of some similarity judgment.



\para{The effect of noise in triplets.}
To test the limitations of \mtl, we perturb human judgment by adding noise to triplets. 
We add noise to triplets by randomly flipping the similarity judgement (i.e., a triplet $(x^r, x^+, x^-)$ becomes $(x^r, x^-, x^+)$) with probability $p$.
As shown in \figref{fig:noise}, direct comparison results decreases linearly as noise increases, decision support performance does not start decreasing util $p=0.5$. 
This is not as surprising for two reasons. First, since VW is a binary dataset, half of the triplets are ones where $x^+, x^-$ belong to the same class; 
flipping these triplets does not have a significant effect on the generated embeddings. 
More importantly, filtering greatly reduces the adversarial effect of adding noise, highlighting the importance of filtering. 
In comparison, \tn drops linearly in NINO. \mtl trained on unfiltered triplets is also more vulnerable to noise perturbations. H2H and decision support performance is overall worse and decreases much faster than that of filtered \mtl.


\para{Number of triplets} In ~\figref{fig:vary-num} we examine the effect of the number of triplets. We decrease number of triplets by powers of 2 
and find that H2H preference towards \mtl indeed declines as \mtl representation is less human-compatible with fewer training data.
As for decision support, in NINO, \mtl declines and eventually approaches \resn except the outlier at end, while in NIFO, \mtl is able to stay 100\% even as the number of triplets declines. 


\subsubsection{Hyperparameter Tuning}

\paragraph{Results for different $\lambda$.}
We show results for $\lambda=0.2$ in Table~\ref{tab:wv_square_filtered_l=0.2} and $\lambda=0.8$ in Table~\ref{tab:wv_square_filtered_l=0.8}. We do not observe a clear trend between $\lambda$ and evaluation metric performances. In the previous section we present \mtl with $\lambda=0.5$ as it shows best overall performance.

\paragraph{Results for 50-dimension embedding.} Table~\ref{tab:wv_square_filtered_l=0.5_d=50} shows results with all models outputing a 50-dimension embedding. Similar to $\lambda$, 50-dimension embedding gives slightly worse resutls, but the trend between embedding dimension and our evaluation metrics is not clear.

\paragraph{Results for unfiltered \mtl.} 
In table~\ref{tab:wv_square_unfiltered_l=0.5} we show results on \mtl with $\lambda=0.5$ using unfiltered triplets. As cocncluded previously, an unfiltered \mtl performs much worse in NINO decision support but slightly better in H2H.

 
  \begin{figure}[t!]
    \center
    \includegraphics[width=0.31\textwidth]{figures/supp/linear_dist.png}
    \caption{VW dataset with a linear decision boundary.}
    \label{fig:linear-dist}
  \end{figure}

\input{tables/supp/wv_squarelin_filtered_l=0.5_ci.tex}


\subsection{Linear decision boundary experiment results}
We create a linear decision boundary on VW without altering the dataset (\figref{fig:linear-dist}). We find the results are overall similar to the original VW data.

\subsubsection{Results}

\paragraph{H2H and decision support results}
In Table~\ref{tab:wv_lin_filtered_l=0.5} we present results with the best set of hyperparameter: filtered triplets, 512-dimension embedding, $\lambda=0.5$. Similar to the experiment on VW double-square decision boundary experiments, we observe that \mtl outperforms \resn in both H2H comparison and decision support performance. 

We show results for $\lambda=0.2$ in Table~\ref{tab:wv_lin_filtered_l=0.2} and $\lambda=0.8$ in Table~\ref{tab:wv_lin_filtered_l=0.8}. We also show results with 50-dimension \mtl in Table~\ref{tab:wv_lin_filtered_l=0.5_d=50} and unfiltered \mtl in Table~\ref{tab:wv_lin_unfiltered_l=0.5}. 
Similar to the experiment on VW double-square decision boundary experiments, we see no clear relation between $\lambda$, embedding dimension and our evaluation metrics. Table 16 again shows that filtered \mtl leads to better decision support but worse H2H.


\input{tables/supp/wv_squarelin_filtered_l=0.2_ci.tex}

% \input{tables/supp/wv_squarelin_filtered_l=0.8_ci.tex}

\input{tables/supp/wv_squarelin_filtered_l=0.5_d=50.tex}

% \input{tables/supp/wv_squarelin_unfiltered_l=0.5_ci.tex}

\paragraph{The effect of perturbations} Similar to previous results, adding noise harms H2H and decision support performance with unfiltered \mtl more so than filtered (~\figref{fig:lin-noise-filtered}, ~\figref{fig:lin-noise-unfiltered}). ~\figref{fig:lin-num} shows H2H and decision support performance with decreasing number of triplets; results are similar to VW double-square decision boundary data.

In conclusion, from two sets of synthetic data with different decision boundaries, we see \mtl overall outperforms our baselines \resn and \tn in H2H and decision support.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_filtered_h2h.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_filtered_NINO.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_filtered_NIFO.pdf}
      \end{subfigure}
      \caption{Results on VW linear decision boundary data with varying noise levels. \mtl uses filtered triplets.}
      \label{fig:lin-noise-filtered}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_unfiltered_h2h.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_unfiltered_NINO.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_noise_unfiltered_NIFO.pdf}
      \end{subfigure}
      \caption{Results on VW linear decision boundary data with varying noise levels. \mtl uses unfiltered triplets.}
      \label{fig:lin-noise-unfiltered}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_num_h2h.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_num_NINO.pdf}
      \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/supp/wv_squarelin_num_NIFO.pdf}
      \end{subfigure}
      \caption{Results on VW linear decision boundary data with varying number of triplets. \mtl uses filtered triplets.}
      \label{fig:lin-num}
\end{figure}

